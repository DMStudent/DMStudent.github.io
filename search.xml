<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CS224n笔记4 WordWindow分类与神经网络]]></title>
    <url>%2F2018%2F11%2F23%2FCS224n%E7%AC%94%E8%AE%B04-WordWindow%E5%88%86%E7%B1%BB%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[CS224n笔记4 WordWindow分类与神经网络 Window classification这是一种根据上下文给单个单词分类的任务，可以用于消歧或命名实体分类。上下文Window的向量可以通过拼接所有窗口中的词向量得到： 最简单的分类器：softmax \hat{y_{y}}=p(y|x)=\frac{e^{W_yx}}{\sum_{c=1}^{C}e^{W_cx}}放大 J(\theta)=\frac{1}{N}\sum_{i=1}^{N}-y_ilog\cdot\frac{e^{W_yx}}{\sum_{c=1}^{C}e^{W_cx}}J对x求导，注意这里的x指的是窗口所有单词的词向量拼接向量。 \begin{aligned} &\Delta_xJ =\frac{\partial}{\partial x}-\log softmax\left(f_y(x)\right)\\ &= \sum_{c=1}^C-\frac{\partial \log softmax\left(f_y(x)\right)}{\partial f_c}\cdot \frac{\partial f_c(x)}{\partial x}\\ &=\left[ \begin{array}{c} \hat{y_1}\\ \vdots\\ \hat{y}-1\\ \vdots\\ \hat{y_C} \end{array} \right]\cdot \frac{\partial f_c(x)}{\partial x}\\ &=\delta \cdot \frac{\partial f_c(x)}{\partial x}\\ &=\sum_{c=1}^C \delta_c W_c^T\\ &=W^T\delta \in \mathbb{R}^{5d} \end{aligned}于是就可以更新词向量了： \nabla_{\theta} J(\theta) = \left[ \begin{array}{c} \nabla_{x_{museums}} \\ \vdots \\ \nabla_{x_{amazing}} \end{array} \right]另一方面，对$W$求偏导数，将所有参数的偏导数写到一起有： \nabla_{\theta} J(\theta) = \left[ \begin{array}{c} \nabla_{W_{\cdot 1}} \\ \vdots \\ \nabla_{W_{\cdot d}} \\ \nabla_{x_{aardvark}} \\ \vdots \\ \nabla_{x_{zebra}} \end{array} \right] \in \mathbb{R}^{Cd+Vd}在更新过程中计算代价最高的有矩阵运算$f=Wx$和指数函数，但矩阵运算比循环要快多了（在CPU上快一个数量级）。 softmax（等价于逻辑斯谛回归）效果有限 使用神经网络神经网络可以提供非线性的决策边界： 一个简单的网络 间隔最大化目标函数minimize J = \max(1+s_c - s, 0)$s_c$代表误分类样本的得分，$s$表示正确分类样本的得分。 反向传播训练 另外，对偏置的偏导数是: 最后一片拼图是对词向量的偏导数，由于连接时每个输入单元连到了多个隐藏单元，所以对某个输入单元的偏导数是求和的形式（残差来自相连的隐藏单元）： 其中，$W^T_j$是第$j$列，转置后得到行向量；红色部分是误差，相乘后得到一个标量，代表词向量第$j$维的导数。那么对整个词向量的偏导数就是： \frac{\partial s}{\partial x}=W^T\delta]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS224n笔记3 高级词向量表示]]></title>
    <url>%2F2018%2F11%2F21%2FCS224n%E7%AC%94%E8%AE%B03-%E9%AB%98%E7%BA%A7%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[CS224n笔记3 高级词向量表示负采样词表V的量级非常大，以至于下式的分母很难计算： p(o|c)=\frac{e^{u_{o}^{T}V_{c}}}{\sum_{1}^{v}{e^{u_{w}^{T}V_{c}}}} 采样子集简化运算的方法，对每个正例（中央词语及上下文中的一个词语）采样几个负例（中央词语和其他随机词语），训练binary logistic regression（也就是二分类器）。 目标函数： J_t(\theta) = \log \sigma\left(u_o^Tv_c\right)+\sum_{i=1}^k\mathbb{E}_{j \sim P(w)}\left [\log\sigma \left(-u^T_jv_c\right) \right]这里t是某个窗口，k是采样个数，P(w)是一个unigram分布，σ是sigmoid函数。 P(w)=\frac{[counter(w)]^{0.75}}{\displaystyle\sum_{u\in D}[counter(u)]^{0.75}}$J_t(\theta)$最大化中央词与上下文的相关概率，最小化与其他词语的概率。 其他方法word2vec将窗口视作训练单位，每个窗口或者几个窗口都要进行一次参数更新。要知道，很多词串出现的频次是很高的。能不能遍历一遍语料，迅速得到结果呢？ 早在word2vec之前，就已经出现了很多得到词向量的方法，这些方法是基于统计共现矩阵的方法。如果在窗口级别上统计词性和语义共现，可以得到相似的词。如果在文档级别上统计，则会得到相似的文档（潜在语义分析LSA）。 基于窗口的共现矩阵比如窗口半径为1，在如下句子上统计共现矩阵： I like deep learning. I like NLP. I enjoy flying. 会得到：朴素共现向量的问题根据这个矩阵，的确可以得到简单的共现向量。但是它存在非常多的局限性： 当出现新词的时候，以前的旧向量连维度都得改变 高纬度（词表大小） 高稀疏性 解决办法：降维SVD改进 限制高频词的频次，或者干脆停用词 根据与中央词的距离衰减词频权重 用皮尔逊相关系数代替词频 效果方法虽然简单，但效果也还不错：SVD的问题 计算复杂度高：对$n×m$的矩阵是$O(mn^2)$ 不方便处理新词或新文档 与其他DL模型训练套路不同 Count based vs direct prediction这些基于计数的方法在中小规模语料训练很快，有效地利用了统计信息。但用途受限于捕捉词语相似度，也无法拓展到大规模语料。 而NNLM, HLBL, RNN, Skip-gram/CBOW这类进行预测的模型必须遍历所有的窗口训练，也无法有效利用单词的全局统计信息。但它们显著地提高了上级NLP任务，其捕捉的不仅限于词语相似度。 综合两者优势：GloVe目标函数是 J(\theta)=\frac{1}{2}\sum_{i,j=1}^W f(P_{ij})\left(u_i^Tv_j-\log P_{ij}\right)^2这里的$P_{ij}$是两个词共现的频次，f是一个max函数： 优点是训练快，可以拓展到大规模语料，也适用于小规模语料和小向量。 明眼人会发现这里面有两个向量$u$和$v$，它们都捕捉了共现信息，怎么处理呢？试验证明，最佳方案是简单地加起来： X_{final} =U+V相对于word2vec只关注窗口内的共现，GloVe这个命名也说明这是全局的（我觉得word2vec在全部语料上取窗口，也不是那么地local，特别是负采样）。 CBOWCBOW 是 Continuous Bag-of-Words Model 的缩写，是一种根据上下文的词语预测当前词语的出现概率的模型。 CBOW是根据上下文预测当中的一个词，也就是用多个词预测一个词 比如这样一个句子yesterday was really a […] day，中间可能是good也可能是nice，比较生僻的词是delightful。当CBOW去预测中间的词的时候，它只会考虑模型最有可能出现的结果，比如good和nice，生僻词delightful就被忽略了。 而对于[…] was really a delightful day这样的句子，每个词在进入模型后，都相当于进行了均值处理（权值乘以节点），delightful本身因为生僻，出现得少，所以在进行加权平均后，也容易被忽视。 Skip-Gram是根据一个词预测它的上下文，也就是用一个词预测多个词，每个词都会被单独得训练，较少受其他高频的干扰。所以对于生僻词Skip-Gram的word2vec更占优。 Hierarchical Softmax输出层输出最可能的w。由于语料库中词汇量是固定的|C|个，所以上述过程其实可以看做一个多分类问题。给定特征，从|C|个分类中挑一个。 对于神经网络模型多分类，最朴素的做法是softmax回归。softmax回归需要对语料库中每个词语（类）都计算一遍输出概率并进行归一化，在几十万词汇量的语料上无疑是令人头疼的。 上图输出层的树形结构即为Hierarchical Softmax。非叶子节点相当于一个神经元，二分类决策输出1或0，分别代表向下左转或向下右转；每个叶子节点代表语料库中的一个词语，于是每个词语都可以被01唯一地编码，并且其编码序列对应一个事件序列，于是我们可以计算条件概率: p(w|Context(w))=\prod_{j=2}^{l^w}p(d_j^w|X_w,\theta_{j-1}^w)其中，$l^w$表示路径中节点个数，$d_j^w\in\{0,1\}$表示路径第j个节点对应的编码，$\theta_j^w$表示非叶节点对应的参数向量。其中，每一项是一个逻辑斯谛回归： p(w|Context(w))=[\sigma(X_w^T\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(X_w^T\theta_{j-1}^w)]^{d_j^w}词语义项的线性代数结构与词义消歧词向量本身无法解决一词多义的问题，比如：tie可能表示球赛的平局，也可能表示领带，还可能表示绳子打结。那它的词向量究竟在哪里呢？ 虽然相似的词被映射到邻近的位置，但该论文证明词向量是所有义项的平均。 复原linear algebraic structure of word senses with applications to polysemy 研究发现义项是由如下sparse coding编码： v=\sum_{i=0}^{D}\alpha_iA_i+\eta这里的A是类似于sports之类的Context vectors（或说义项的vector），$\alpha$是某个Context vector的系数（hard sparsity constraint）。论文中说这些参数可以通过标准k-SVD算法求出，$\alpha$表示噪声。 量化评测是请了各国的研究生与该方法一起作为实验对象，做如下的题目：问他们这些词语与某个词是否相关，计算PR值。结果证明，这种方法可以达到non-native speaker的水平。 词向量是多个义项的叠加。通过复原方法，可以通过sparse coding的k-SVD算法复原词向量中的所有义项。复原水平达到non-nativ]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n笔记2 词的向量表示]]></title>
    <url>%2F2018%2F11%2F21%2FCS224n%E7%AC%94%E8%AE%B02-%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[CS224n笔记2 词的向量表示word embeddings的基本思路定义一个以预测某个单词的上下文的模型： p = (context|w_{t})=... 损失函数定义如下： J=1-p(w_{-t}|w_{t})这里的$w_{-t}$表示$w_{t}$的上下文（负号通常表示除了某某之外），如果完美预测，损失函数为零。 然后在一个大型语料库中的不同位置得到训练实例，调整词向量，最小化损失函数。 两个算法 Skip-grams (SG)：预测上下文 Continuous Bag of Words (CBOW)：预测目标单词 两种高效一些的训练方法： Hierarchical softmax Negative sampling Skipgram 这两个矩阵都含有V个词向量，也就是说同一个词有两个词向量，哪个作为最终的、提供给其他应用使用的embeddings呢？有两种策略，要么加起来，要么拼接起来。在CS224n的编程练习中，采取的是拼接起来的策略。 word2vec细节目标函数定义为所有位置的预测结果的乘积的负对数： J(\theta)=-\frac{1}{T}\sum_{t=1}^T{\sum_{-m, j\neq0}^{m}{log p(w_{t+j}|w_{t})}}p(o|c)=\frac{e^{u_{o}^{T}V_{c}}}{\sum_{1}^{v}{e^{u_{w}^{T}V_{c}}}} 交叉熵:任何一个由负对数似然组成的损失都是定义在训练集上的经验分布和定义在模型上的概率分布之间的交叉熵。 梯度 梯度下降、SGD\theta^{new}=\theta^{old}-\alpha\frac{\partial}{\partial\theta^{old}}J(\theta)Sentence Embedding 计算相似度 情感分析 特征 生成模型模型假设: 语料的生成是一个动态过程(dynamic process), 即第t个单词是在第t步生成的。每个单词w对应着一个$\mathbb{R}^d$维的向量。 而这个动态过程是由discourse vector$c_t\in{\mathbb{R}^d}$的随机游走驱动的。discourse vector代表着这个句子的一个状态, 由于是动态的, 这个状态是随时间变化的。 假设t时刻观测到单词w的概率为单词与整个句子内积的对数线性(log linear)关系: Pr(\text{w emitted at time t}| c_t)\propto{\exp(\langle c_t,v_w \rangle)}相邻的单词是由近似的discourse vector生成得到的，因此通过这种办法生成的单词向量, 与word2vec(CBOW)和Glove生成的向量是相似的。 随机游走模型的改进A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SEN- TENCE EMBEDDINGS为了简化, 注意到$c_{t}$在整个句子生成单词的过程中, 变化很小, 因此我们将所有步的discourse vector假设为一个固定的向量$c_{s}$。 可证明: 对cs的最大似然估计就是对所有单词embedding向量的平均。 这篇论文对这种模型进行了改进, 加入了两项平滑项, 出于如下的考虑: 有些单词在规定的上下文范围之外出现, 也可能对discourse vector产生影响 有限单词的出现(如常见的停止词)与discourse vector没有关系 单词$w$在句子$s$中出现的概率为 Pr(\text{w emitted in sentence s}| c_s)\propto{\alpha p(w) + (1-\alpha)\frac{\exp(\langle \tilde{c}_s, v_w \rangle)}{Z_{\tilde{c}_s}}}其中$\tilde{c}_s=\beta c_0+(1-\beta)c_s,\ c_0\perp c_s$，$c_{0}$是句子的最频繁的意义, 可以认为是句子中最重要的成分,$Z_{\tilde{c}_s}=\sum\limits_{w\in{V}}\exp(\langle \tilde{c}_s, v_w \rangle)$是归一化常数，$p(w)$是单词w在整个语料中出现的概率(词频角度)。 计算句子向量使用最大似然法估计$c_s$向量。 首先假设所有单词的向量$v_s$是大致均匀分布在整个向量空间上的, 因此这里的归一化项$Z_c$对于不同的句子值都是大致相同的, 即对于任意的$\tilde{c}_s$, $Z$值是相同的. 在此前提下, 得到似然函数: p[s|c_s]=\prod\limits_{w\in{s}}p(w|c_s)=\prod\limits_{w\in{s}}[\alpha p(w) + (1-\alpha)\frac{\exp(\langle \tilde{c}_s, v_w \rangle)}{Z}]取对数, 单个单词记为 f_w(\tilde{c}_s)=\log[\alpha p(w) + (1-\alpha)\frac{\exp(\langle \tilde{c}_s, v_w \rangle)}{Z}]最大化上式, 具体的推到在论文中有详述的说明, 最终目标为: \arg\max\limits_{c_s}\sum\limits_{w\in{s}}f_w(\tilde{c}_s)可以得到: \sum\limits_{w\in{s}}\frac{a}{p(w)+a}v_w,\ a=\frac{1-\alpha}{\alpha Z} 最优解为句子中所有单词向量的加权平均 对于词频更高的单词w, 权值更小, 因此这种方法也等同于下采样频繁单词 最后, 为了得到最终的句子向量$c_s$, 我们需要估计$c_0$。 通过计算向量$\tilde{c}_s$的first principal component(PCA中的主成分), 将其作为$c_0$. 最终的句子向量即为$\tilde{c}_s$减去主成份向量$c_0$。 整个算法步骤总结如下图：]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n笔记1 自然语言处理与深度学习简介]]></title>
    <url>%2F2018%2F11%2F21%2FCS224n%E7%AC%94%E8%AE%B01-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[CS224n笔记1 自然语言处理与深度学习简介这是斯坦福CS224n的第一篇笔记，也是第一次系统地学习用深度学习来做自然语言处理。本文还整理了CS224n的全部视频课件笔记。相关资源： 旧版CS224d：http://cs224d.stanford.edu/ 新版CS224n官网：http://web.stanford.edu/class/cs224n/index.html 新版CS224n官方笔记：https://github.com/stanfordnlp/cs224n-winter17-notes 新版CS224n视频地址：https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6 新版CS224n国内在线观看：http://space.bilibili.com/34967/#!/channel/detail?cid=18192&amp;order=1&amp;page=1 视频课件笔记 什么是自然语言处理这是一门计算机科学、人工智能以及语言学的交叉学科。虽然语言只是人工智能的一部分（人工智能还包括计算机视觉等），但它是非常独特的一部分。这个星球上有许多生物拥有超过人类的视觉系统，但只有人类才拥有这么高级的语言。 自然语言处理的目标是让计算机处理或说“理解”自然语言，以完成有意义的任务，比如订机票购物或QA等。完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。 自然语言处理涉及的几个层次 作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词（事实上，跳过分词虽然理所当然地不能做句法分析，但字符级也可以直接做不少应用）。接下来是形态学，援引《统计自然语言处理》中的定义： 形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科［Matthews,2000］。 下面的是句法分析和语义分析，最后面的在中文中似乎翻译做“对话分析”，需要根据上文语境理解下文。 这门课主要关注画圈的三个部分，其中中间的两个是重中之重，虽然深度学习在语音识别上的发力最大。 自然语言处理应用 拼写检查、关键词检索…… 文本挖掘（产品价格、日期、时间、地点、人名、公司名） 文本分类 机器翻译 客服系统 复杂对话系统 人类语言的特殊之处与信号处理、数据挖掘不同，自然语言的随机性小而目的性强；语言是用来传输有意义的信息的，这种传输连小孩子都能很快学会。人类语言是离散的、明确的符号系统。但又允许出现各种变种，比如颜文字，随意的错误拼写“I loooove it”。这种自由性可能是因为语言的可靠性（赘余性）。所以说语言文字绝对不是形式逻辑或传统AI的产物。 语言符号有多种形式（声音、手势、书写），在这些不同的形式中，其意义保持不变。 虽然人类语言是明确的符号系统，但符号传输到大脑的过程是通过连续的声学光学信号，大脑编码似乎是连续的激活值上的模式。另外巨大的词表也导致数据稀疏，不利于机器学习。这构成一种动机，是不是应该用连续的信号而不是离散的符号去处理语言。 什么是深度学习这是机器学习的一个子集。传统机器学习中，人类需要对专业问题理解非常透彻，才能手工设计特征。然后把特征交给某个机器学习算法，比如线性分类器。机器为这些特征调整找到合适的权值，将误差优化到最小。 在这个过程中一直在学习的其实是人类，而不是机器。机器仅仅做了一道数值优化的题目而已。 而深度学习是表示学习的一部分，用来学习原始输入的多层特征表示： 为什么需要研究深度学习 手工特征耗时耗力，还不易拓展 自动特征学习快，方便拓展 深度学习提供了一种通用的学习框架，可用来表示世界、视觉和语言学信息 深度学习既可以无监督学习，也可以监督学习 为什么NLP难人类语言是充满歧义的，不像编程语言那样明确。编程语言中有各种变量名，但人类语言中只有少数几个代词可以用，你得思考到底指代的是谁…… 人类语言的解读依赖于现实世界、常识以及上下文。由于说话速度书写速度阅读速度的限制，人类语言非常简练，省略了大量背景知识。 接下来是几个英文的歧义例子，对native speaker而言很有趣。为了完整性只看一个： The Pope’s baby steps on gays 主要歧义发生在baby上面，可以理解为“教皇的孩子踩了基佬”，也可以理解为“教皇在同性恋问题上裹足不前”。 Deep NLP = Deep Learning + NLP将自然语言处理的思想与表示学习结合起来，用深度学习的手法解决NLP目标。这提高了许多方面的效果： 层次：语音、词汇、语法、语义 工具：词性标注、命名实体识别、句法\语义分析 应用：机器翻译、情感分析、客服系统、问答系统 深度学习的一个魅力之处是，它提供了一套“宇宙通用”的框架解决了各种问题。虽然工具就那么几个，但在各行各业都适用。 NLP表示层次：形态级别传统方法在形态级别的表示是词素： 深度学习中把词素也作为向量：多个词素向量构成相同纬度语义更丰富的词向量。 NLP工具：句法分析 NLP语义层面的表示传统方法是手写大量的规则函数，叫做Lambda calculus：在深度学习中，每个句子、短语和逻辑表述都是向量。神经网络负责它们的合并。 情感分析传统方法是请一两百个工人，手工搜集“情感极性词典”在词袋模型上做分类器。 深度学习复用了RNN来解决这个问题，它可以识别“反话”的情感极性： 注意这只是为了方便理解的示意图，并不是RNN的工作流程。私以为这张图放在这里不合适，可能会误导一部分人，以为神经网络就是这样的基于规则的“决策树”模型。 QA传统方法是手工编写大量的逻辑规则，比如正则表达式之类。深度学习依然使用了类似的学习框架，把事实储存在向量。 客服系统GMail的自动回复 机器翻译传统方法在许多层级上做了尝试，词语、语法、语义之类。这类方法试图找到一种世界通用的“国际语”（Interlingua）来作为原文和译文的桥梁。 而Neural Machine Translation将原文映射为向量，由向量构建译文。也许可以说Neural Machine Translation的“国际语”是向量。 结论：所有层级的表示都是向量这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高阶的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K近邻]]></title>
    <url>%2F2018%2F11%2F14%2FK%E8%BF%91%E9%82%BB%2F</url>
    <content type="text"><![CDATA[K近邻]]></content>
      <categories>
        <category>机器学习</category>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感知机]]></title>
    <url>%2F2018%2F11%2F14%2F%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[感知机]]></content>
      <categories>
        <category>机器学习</category>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学习方法概述]]></title>
    <url>%2F2018%2F11%2F08%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[统计学习方法概述]]></content>
      <categories>
        <category>机器学习</category>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习方法</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub+Hexo搭建个人网站详细教程]]></title>
    <url>%2F2018%2F11%2F07%2FGitHub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[GitHub+Hexo 搭建个人网站详细教程前言：随着互联网浪潮的翻腾，国内外涌现出越来越多优秀的社交网站让用户分享信息更加便捷。然后，如果你是一个不甘寂寞的程序猿（媛），是否也想要搭建一个属于自己的个人网站，如果你曾经或者现在正有这样的想法，请跟随这篇文章发挥你的Geek精神，让你快速拥有自己的博客网站，写文章记录生活，享受这种从0到1的过程。 你见过的最棒的个人博客界面是什么样的？什么是Hexo ? Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Heroku上，是搭建博客的首选框架。这里我们选用的是GitHub，你没看错，全球最大的同性恋交友网站（逃……）。Hexo同时也是GitHub上的开源项目，参见：hexojs/hexo 如果想要更加全面的了解Hexo，可以到其官网Hexo了解更多的细节，因为Hexo的创建者是台湾人，对中文的支持很友好，可以选择中文进行查看。这里，默认各位猿/媛儿都知道GitHub就不再赘述。 下面正式从零开始搭建年轻人的第一个网站。 搭建步骤： 获得个人网站域名 GitHub创建个人仓库 安装Git 安装Node.js 安装Hexo 推送网站 绑定域名 更换主题 初识MarkDown语法 发布文章 寻找图床 个性化设置 其他 附录 获得个人网站域名域名是网站的入口，也是网站的第一印象，比如饿了么的官网的域名是：https://www.ele.me/ ，很是巧妙。常见的有com,cn,net,org等后缀，也有小众的xyz,me,io等后缀，根据你自己的喜好，选择不同的后缀，比如我选择就是常见的com后缀。很多小众奇特的后缀在大陆是没办法备案的，网站也就无法上线。然而使用GitHub托管我们的网站，完全不需要备案，因为托管我们的网站内容的服务器在美国，而且在国内备案流程也比较繁杂，时间需要一周左右。 申请域名的地方有很多，这里推荐阿里云：阿里云-为了无法计算的价值购买域名,这也是我们整个搭建过程中惟一一个需要花钱的地方。如果你已经有了空闲域名就无需购买，直接使用即可。 GitHub创建个人仓库登录到GitHub,如果没有GitHub帐号，使用你的邮箱注册GitHub帐号：Build software better, together 点击GitHub中的New repository创建新仓库，仓库名应该为：用户名.github.io 这个用户名使用你的GitHub帐号名称代替，这是固定写法，比如我的仓库名为： dmstudent.github.io 安装Git什么是Git ?简单来说Git是开源的分布式版本控制系统，用于敏捷高效地处理项目。我们网站在本地搭建好了，需要使用Git同步到GitHub上。如果想要了解Git的细节，参看廖雪峰老师的Git教程：Git教程 从Git官网下载：Git - Downloading Package 现在的机子基本都是64位的，选择64位的安装包，下载后安装，在命令行里输入git测试是否安装成功，若安装失败，参看其他详细的Git安装教程。安装成功后，将你的Git与GitHub帐号绑定，鼠标右击打开Git Bash，或者在菜单里搜索Git Bash，设置user.name和user.email配置信息：12git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot; 生成ssh密钥文件：1ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; 然后直接三个回车即可，默认不需要设置密码然后找到生成的.ssh的文件夹中的id_rsa.pub密钥，将内容全部复制打开GitHub_Settings_keys 页面，新建new SSH KeyTitle为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴进去，最后点击Add SSH key。在Git Bash中检测GitHub公钥设置是否成功，输入 ssh git@github.com ：如上则说明成功。这里之所以设置GitHub密钥原因是，通过非对称加密的公钥与私钥来完成加密，公钥放置在GitHub上，私钥放置在自己的电脑里。GitHub要求每次推送代码都是合法用户，所以每次推送都需要输入账号密码验证推送用户是否是合法用户，为了省去每次输入密码的步骤，采用了ssh，当你推送的时候，git就会匹配你的私钥跟GitHub上面的公钥是否是配对的，若是匹配就认为你是合法用户，则允许推送。这样可以保证每次的推送都是正确合法的。 Coding创建个人仓库登录到Coding,如果没有Coding帐号，使用你的邮箱注册Coding帐号：Coding 点击在Co创建一个项目，仓库名应该为：用户名 这个用户名使用你的Coding帐号名称代替，这是固定写法，比如我的仓库名为： dmstudent 配置CodingGit的SSH和github类似，打开个人中心的SSH公匙。 添加后测试一下1ssh -T git@git.coding.net 设置Coding项目中的配置 必须要在source/新建一个空白文件,名字必须是Staticfile 在刚刚建的项目中代码模块开启pages功能,这里的部署分支选择master,因为你在_config.yml中设置的分支是master,然后点击立即开启. 安装Node.jsHexo基于Node.js，Node.js下载地址：Download | Node.js 下载安装包，注意安装Node.js会包含环境变量及npm的安装，安装后，检测Node.js是否安装成功，在命令行中输入 node -v :检测npm是否安装成功，在命令行中输入npm -v :到这了，安装Hexo的环境已经全部搭建完成。 安装HexoHexo就是我们的个人博客网站的框架， 这里需要自己在电脑常里创建一个文件夹，可以命名为Blog，Hexo框架与以后你自己发布的网页都在这个文件夹中。创建好后，进入文件夹中，按住shift键，右击鼠标点击命令行使用npm命令安装Hexo，输入：1npm install -g hexo-cli 这个安装时间较长耐心等待，安装完成后，初始化我们的博客，输入：1hexo init blog 注意，这里的命令都是作用在刚刚创建的Blog文件夹中。 为了检测我们的网站雏形，分别按顺序输入以下三条命令： 123hexo new test_my_sitehexo ghexo s 这些命令在后面作介绍，完成后，打开浏览器输入地址：1localhost:4000 可以看出我们写出第一篇博客，只不过我下图是我修改过的配置，和你的显示不一样。现在来介绍常用的Hexo 命令 npm install hexo -g #安装Hexonpm update hexo -g #升级hexo init #初始化博客 命令简写hexo n “我的博客” == hexo new “我的博客” #新建文章hexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署 hexo server #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存，若是网页正常情况下可以忽略这条命令 刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作。 推送网站上面只是在本地预览，接下来要做的就是就是推送网站，也就是发布网站，让我们的网站可以被更多的人访问。在设置之前，需要解释一个概念，在blog根目录里的_config.yml文件称为站点配置文件，如下图进入根目录里的themes文件夹，里面也有个_config.yml文件，这个称为主题配置文件，如下图下一步将我们的Hexo与GitHub关联起来，打开站点的配置文件_config.yml，翻到最后修改为：保存站点配置文件。 其实就是给hexo d 这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。最后安装Git部署插件，输入命令：1npm install hexo-deployer-git --save 这时，我们分别输入三条命令：123hexo clean hexo g hexo d 其实第三条的 hexo d 就是部署网站命令，d是deploy的缩写。完成后，打开浏览器，在地址栏输入你的放置个人网站的仓库路径，即 http://xxxx.github.io 。 你就会发现你的博客已经上线了，可以在网络上被访问了。 绑定域名 虽然在Internet上可以访问我们的网站，但是网址是GitHub提供的:http://xxxx.github.io (知乎排版可能会出现”http://&quot;字样) 而我们想使用我们自己的个性化域名，这就需要绑定我们自己的域名。这里演示的是在阿里云万网的域名绑定，在国内主流的域名代理厂商也就阿里云和腾讯云。登录到阿里云，进入管理控制台的域名列表，找到你的个性化域名，进入解析然后添加解析包括添加三条解析记录，192.30.252.153是GitHub的地址，你也可以ping你的 http://xxxx.github.io 的ip地址，填入进去。第三个记录类型是CNAME，CNAME的记录值是：你的用户名.http://github.io 这里千万别弄错了。第二步，登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名点击save保存。第三步，进入本地博客文件夹 ，进入blog/source目录下，创建一个记事本文件，输入你的域名，对，只要写进你自己的域名即可。如果带有www，那么以后访问的时候必须带有www完整的域名才可以访问，但如果不带有www，以后访问的时候带不带www都可以访问。所以建议，不要带有www。这里我还是写了www(不建议带有www):保存，命名为CNAME ，注意保存成所有文件而不是txt文件。 完成这三步，进入blog目录中，按住shift键右击打开命令行，依次输入：123hexo cleanhexo ghexo d 这时候打开浏览器在地址栏输入你的个性化域名将会直接进入你自己搭建的网站。 更换主题如果你不喜欢Hexo默认的主题，可以更换不同的主题，主题传送门：Themes 我自己使用的是Next主题，可以在blog目录中的themes文件夹中查看你自己主题是什么。现在把默认主题更改成Next主题，在blog目录中（就是命令行的位置处于blog目录）打开命令行输入：1git clone https://github.com/iissnan/hexo-theme-next themes/next 这是将Next主题下载到blog目录的themes主题下的next文件夹中。打开站点的_config.yml配置文件，修改主题为next打开主题的_config.yml配置文件，不是站点主题文件，找到Scheme Settingsnext主题有三个样式，我用的是Pisces，你们可以自己试试看，选择你自己喜欢的样式（只需要把行首的#去除，#是注释），选择好后，再次部署网站，hexo g、hexo d，查看效果。选择其他主题，按照上述过程即可实现。 个性化设置所谓的个性化设置就是根据个人需要添加不同的插件及功能。基本的有： 在站点配置文件_config.yml修改基本的站点信息 依次是网站标题、副标题、网站描述、作者、网站头像外部链接、网站语言、时区等。在主题配置文件_config.yml修改基本的主题信息，如： 分类选项生成“分类”页并添加tpye属性打开命令行，进入博客所在文件夹。执行命令1$ hexo new page categories 成功后会提示：1INFO Created: ~/Documents/blog/source/categories/index.md 根据上面的路径，找到index.md这个文件，打开后默认内容是这样的：1234---title: 文章分类date: 2017-05-27 13:47:40--- 添加type: “categories”到内容中，添加后是这样的：12345---title: 文章分类date: 2017-05-27 13:47:40type: &quot;categories&quot;--- 保存并关闭文件。 给文章添加“categories”属性 打开需要添加分类的文章，为其添加categories属性。下方的categories: web前端表示添加这篇文章到“web前端”这个分类。注意：hexo一篇文章只能属于一个分类，也就是说如果在“- web前端”下方添加“-xxx”，hexo不会产生两个分类，而是把分类嵌套（即该文章属于 “- web前端”下的 “-xxx ”分类）。123456---title: jQuery对表单的操作及更多应用date: 2017-05-26 12:12:57categories: - web前端--- 标签选项生成“标签”页并添加tpye属性打开命令行，进入博客所在文件夹。执行命令1$ hexo new page tags 成功后会提示：1INFO Created: ~/Documents/blog/source/tags/index.md 根据上面的路径，找到index.md这个文件，打开后默认内容是这样的：1234---title: 标签date: 2017-05-27 14:22:08--- 添加type: “tags”到内容中，添加后是这样的：12345---title: 文章分类date: 2017-05-27 13:47:40type: &quot;tags&quot;--- 保存并关闭文件。 给文章添加“tags”属性 打开需要添加标签的文章，为其添加tags属性。下方的tags:下方的- jQuery - 表格表单验证就是这篇文章的标签了12345678910---title: jQuery对表单的操作及更多应用date: 2017-05-26 12:12:57categories: - web前端tags:- jQuery- 表格- 表单验证--- 至此，成功给文章添加分类，点击首页的“标签”可以看到该标签下的所有文章。当然，只有添加了tags: xxx的文章才会被收录到首页的“标签”中。 细心的朋友可能已经发现，这两个的设置几乎一模一样！是的，没错，思路都是一样的。所以我们可以打开scaffolds/post.md文件，在tages:上面加入categories:,保存后，之后执行hexo new 文章名命令生成的文件，页面里就有categories:项了。 scaffolds目录下，是新建页面的模板，执行新建命令时，是根据这里的模板页来完成的，所以可以在这里根据你自己的需求添加一些默认值。 LocalSearch搜索安装 hexo-generator-searchdb，在站点的根目录下执行以下命令：1$ npm install hexo-generator-searchdb --save 编辑 站点配置文件，新增以下内容到任意位置：12345search: path: search.xml field: post format: html limit: 10000 编辑 主题配置文件，启用本地搜索功能：123# Local searchlocal_search: enable: true 然后 重新生成 查看:12$ hexo clean$ hexo s -g 这样,搜索功能就添加上了. 在Hexo中渲染MathJax数学公式Hexo 输入数学公式主要通过MathJax 渲染LaTeX 公式实现的，具体开启步骤以及简要语法介绍如下。 安装MathJax1npm install hexo-math --save 在站点配置文件 _config.yml 中添加：123456math: engine: &apos;mathjax&apos; # or &apos;katex&apos; mathjax: # src: custom_mathjax_source config: # MathJax config 在 next 主题配置文件中 themes/next-theme/_config.yml 中将 mathJax 设为 true:12345# MathJax Supportmathjax: enable: true per_page: false cdn: //cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML 在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。 原因 Hexo默认使用”hexo-renderer-marked”引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线’_’代表斜体，会被渲染引擎处理为标签。 因为类Latex格式书写的数学公式下划线 ‘_’ 表示下标，有特殊的含义，如果被强制转换为标签，那么MathJax引擎在渲染数学公式的时候就会出错。例如，$x_i$在开始被渲染的时候，处理为$xi$，这样MathJax引擎就认为该公式有语法错误，因为不会渲染。 类似的语义冲突的符号还包括’*’, ‘{‘, ‘}’, ‘\’等。 解决方法更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改：12// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#$+\-.!_&gt;])/ 这一步是在原基础上取消了对\,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改。 12// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 重新启动hexo（先clean再generate）,问题完美解决。]]></content>
      <categories>
        <category>编程</category>
        <category>前端</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Hexo</tag>
        <tag>网站</tag>
      </tags>
  </entry>
</search>
